<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>scrapy on Awesome Hugo blog</title><link>https://mugeliu.github.io/tags/scrapy/</link><description>Recent content in scrapy on Awesome Hugo blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 21 Apr 2022 17:33:49 +0000</lastBuildDate><atom:link href="https://mugeliu.github.io/tags/scrapy/index.xml" rel="self" type="application/rss+xml"/><item><title>æ·±æ‹·è´ä¸æµ…æ‹·è´é—®é¢˜çš„è®°å½•</title><link>https://mugeliu.github.io/posts/2022-04-21-%E6%B7%B1%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B5%85%E6%8B%B7%E8%B4%9D%E9%97%AE%E9%A2%98%E7%9A%84%E8%AE%B0%E5%BD%95/</link><pubDate>Thu, 21 Apr 2022 17:33:49 +0000</pubDate><guid>https://mugeliu.github.io/posts/2022-04-21-%E6%B7%B1%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B5%85%E6%8B%B7%E8%B4%9D%E9%97%AE%E9%A2%98%E7%9A%84%E8%AE%B0%E5%BD%95/</guid><description>æ·±æµ…æ‹·è´çš„é—®é¢˜è®°å½• ğŸ–‡ï¸ åœ¨æŠ“å–æ’è¡Œæ¦œæ•°æ®æ—¶ï¼Œå‘ç°æŠ“å–çš„æ’è¡Œæ¦œæ•°æ®æ€»æ˜¯æœ€åçš„ä¸€æ¡ã€‚ä½†æ˜¯æ’è¡Œæ¦œçš„æ•°æ®é‡æ˜¯æ²¡æœ‰é—®é¢˜çš„ã€‚åœ¨ä»”ç»†æ£€æŸ¥é€»è¾‘åå‘ç°å¹¶æ²¡æœ‰ä»€ä¹ˆé—®é¢˜ã€‚æœ€åæœç´¢æ’æŸ¥æ˜¯æ·±æµ…æ‹·è´çš„é—®é¢˜ï¼Œé‚è®°å½•ä¸€ä¸‹ã€‚ åŸä»£ç  &amp;#x1f447;
def parse_more_rank(self, response): items = response.meta.get(&amp;#34;items&amp;#34;) url_list = response.xpath(&amp;#39;//a[@class=&amp;#34;figure_pic&amp;#34;]/@href&amp;#39;).getall() ranking_list = response.xpath(&amp;#39;//a[@class=&amp;#34;figure_pic&amp;#34;]/span/text()&amp;#39;).getall() title_list = response.xpath(&amp;#39;//a[@class=&amp;#34;figure_pic&amp;#34;]/following-sibling::strong/a/text()&amp;#39;).getall() cover_id_list = [u.split(&amp;#39;cover/&amp;#39;)[1].split(&amp;#34;.&amp;#34;)[0] for u in url_list] for url,ranking,title,cover_id in zip(url_list,ranking_list,title_list,cover_id_list): items[&amp;#34;ranking&amp;#34;] = ranking items[&amp;#34;title&amp;#34;] = title items[&amp;#34;cover_id&amp;#34;] = cover_id yield scrapy.Request(url, callback=self.parse_detail, meta={&amp;#34;items&amp;#34;: items}, dont_filter=True) ç¨‹åºè¿è¡Œåˆ°forå¾ªç¯éƒ½æ˜¯æ²¡æœ‰é—®é¢˜çš„ã€‚åˆ°ä¸€ä¸ªè§£æå›è°ƒself.parse_detailçš„æ—¶å€™å°±å‡ºç°äº†é—®é¢˜ã€‚å­˜å‚¨çš„æ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ€åä¸€æ¡ã€‚ï¼ˆå› ä¸ºä¸åŒçš„æ’è¡Œæ¦œä¸­æœ‰ç›¸åŒçš„ä½œå“ï¼Œæ‰€ä»¥æ²¡æœ‰è¿‡æ»¤é‡å¤è¯·æ±‚ã€‚ï¼‰
æœ€åæ’æŸ¥æ˜¯ä¼ é€’metaä¼ é€’itemsçš„æ—¶å€™çš„é—®é¢˜ï¼Œæ²¡æœ‰ä½¿ç”¨æ·±æ‹·è´ï¼Œå¯¼è‡´æœ€åè§£æå›è°ƒçš„æ—¶å€™éƒ½æ˜¯åŒä¸€ä¸ªå¯¹è±¡ã€‚
æ›´æ”¹å
yield scrapy.Request(url, callback=self.parse_detail, meta=copy.deepcopy({&amp;#34;items&amp;#34;: items}), dont_filter=True) pythonèµ‹å€¼ã€æ·±ã€æµ…æ‹·è´ å‚è€ƒï¼š
èœé¸Ÿæ•™ç¨‹
ç›´æ¥èµ‹å€¼ï¼š å…¶å®å°±æ˜¯å¯¹è±¡çš„å¼•ç”¨ï¼ˆåˆ«åï¼‰ã€‚
æµ…æ‹·è´(copy)ï¼š æ‹·è´çˆ¶å¯¹è±¡ï¼Œä¸ä¼šæ‹·è´å¯¹è±¡çš„å†…éƒ¨çš„å­å¯¹è±¡ã€‚
æ·±æ‹·è´(deepcopy)ï¼šÂ copy æ¨¡å—çš„ deepcopy æ–¹æ³•ï¼Œå®Œå…¨æ‹·è´äº†çˆ¶å¯¹è±¡åŠå…¶å­å¯¹è±¡ã€‚
2022-08-02æ›´æ–° åœ¨scrapyæ–‡æ¡£ä¸­çœ‹åˆ°äº†å…³äºitemæ‹·è´çš„API
yield scrapy.</description></item></channel></rss>